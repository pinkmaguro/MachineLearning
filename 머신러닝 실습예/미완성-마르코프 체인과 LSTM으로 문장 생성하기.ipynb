{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마르코프 체인은 확률을 기반으로 문장을 이어 붙여 나가는 방법이며, LSTM/RNN 은 머신러닝으로 다음에 위치할 문장을 예측해서 문장을 생성하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마르코프 체인이란?\n",
    "\n",
    "현재 상타만을 기반으로 다음 상태를 선택\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Twitter\n",
    "import urllib.request\n",
    "\n",
    "import os,re,json,random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5382203\n"
     ]
    }
   ],
   "source": [
    "dirname = '../data/text/토지/'\n",
    "files = os.listdir(dirname)\n",
    "\n",
    "text = \"\"\n",
    "for filename in files:\n",
    "    if filename.startswith('토지'):\n",
    "        fullname = os.path.join(dirname, filename)\n",
    "        fp = codecs.open(fullname, 'r', encoding='cp949', errors='ignore')\n",
    "        text += fp.read()\n",
    "print(len(text))\n",
    "\n",
    "toji = os.path.join(dirname, 'toji.txt')\n",
    "f = open(toji, 'w', encoding='utf-16')\n",
    "f.write(text)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = codecs.open(toji, 'r', encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = fp.read()\n",
    "\n",
    "text = text.replace('...', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에서 3 개의 연속된 단어를 가지고 이들의 조합을 key 로하고 빈도수를 value로 하는 dict 를 만든다.\n",
    "\n",
    "def make_dic(words):\n",
    "    tmp = ['@']\n",
    "    dic = {}\n",
    "    for word in words:\n",
    "        tmp.append(word)\n",
    "        if len(tmp) < 3: continue\n",
    "        if len(tmp) > 3: tmp = tmp[1:]\n",
    "        set_word3(dic, tmp)\n",
    "        if word == '.':\n",
    "            tmp = ['@']\n",
    "            continue\n",
    "    return dic\n",
    "\n",
    "def set_word3(dic, s3):\n",
    "    w1, w2, w3 = s3\n",
    "    if not w1 in dic: dic[w1] = {}\n",
    "    if not w2 in dic[w1]: dic[w1][w2] = {}\n",
    "    if not w3 in dic[w1][w2]: dic[w1][w2][w3] = 0\n",
    "    dic[w1][w2][w3] += 1\n",
    "    \n",
    "def make_sentence(dic):\n",
    "    ret = []\n",
    "    if not '@' in dic: return 'no dic'\n",
    "    top = dic['@']\n",
    "    w1 = word_choice(top)\n",
    "    w2 = word_choice(w1)\n",
    "    ret.append(w1)\n",
    "    ret.append(w2)\n",
    "    while True:\n",
    "        w3 = word_choice(dic[w1][w2])\n",
    "        ret.append(w3)\n",
    "        if w3 == '.' : break\n",
    "        w1, w2 = w2, w3\n",
    "    ret = ''.join(ret)\n",
    "    \n",
    "    # 띄어쓰기\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"_callback\":\"\",\n",
    "        \"q\": ret\n",
    "    })\n",
    "    \n",
    "    # 네이버 맞춤법 검사기\n",
    "    data = urllib.request.urlopen(\"https://m.search.naver.com/p/csearch/dcontent/spellchecker.nhn?\" + params)\n",
    "    data = data.read().decode('utf-8')[1:-2]\n",
    "    # '({\"message\":{\"@type\":\"response\",\"@service\":\"DBSearchSpellchecker\",\"\n",
    "    #@version\":\"1.1.0\",\"result\":{ \"errata_count\":0,\"html\":\"아버지 가방에 들어가신다.\"}}});'\n",
    "    # 앞의 ( 와 뒤의  ); 제거\n",
    "    \n",
    "    data = json.loads(data)\n",
    "    data = data['message']['result']['html']\n",
    "    ddata = soup = BeautifulSoup(data, 'html.parser').getText()\n",
    "    return data\n",
    "    \n",
    "def word_choice(sel):\n",
    "    keys = sel.keys()\n",
    "    return random.choice(list(keys))\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'temp' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-18682388bdf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-eb2d799665df>\u001b[0m in \u001b[0;36mmake_dic\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mset_word3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'temp' referenced before assignment"
     ]
    }
   ],
   "source": [
    "twitter = Twitter()\n",
    "mallist = twitter.pos(text, norm=True)\n",
    "words = []\n",
    "\n",
    "dict_file = os.path.join(dirname, 'markov-toji.json')\n",
    "for word in mallist:\n",
    "    if not word[1] in ['Punctuation']:\n",
    "        words.append(word[0])\n",
    "    if word[0] == '.':\n",
    "        words.append(word[0])\n",
    "    \n",
    "    dic = make_dic(words)\n",
    "    json.dump(dic, open(dict_file, 'w', encoding='utf-8'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = json.load(open(dic_file, 'r'))\n",
    "\n",
    "for i in range(3):\n",
    "    s = make_sentence(dic)\n",
    "    print(s)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naver(text):\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"_callback\":\"\",\n",
    "        \"q\": text\n",
    "    })\n",
    "    data = urllib.request.urlopen(\"https://m.search.naver.com/p/csearch/dcontent/spellchecker.nhn?\" + params)\n",
    "    data = data.read().decode('utf-8')[1:-2]\n",
    "    data = json.loads(data)\n",
    "    data = data['message']['result']['html']\n",
    "    ddata = soup = BeautifulSoup(data, 'html.parser').getText()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"그러면 <span class='re_red'>안 되나요?</span>\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver('그러면 않되나요?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM/RNN\n",
    "\n",
    "LSTM (Log Short Term-Memory)\n",
    "\n",
    "RNN (Recurrent Neural Network)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
